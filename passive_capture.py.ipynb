{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# passive capture\n",
    "================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## development\n",
    "\n",
    "'passive capture' is my first pass at a GeoDjango  application. Regrettably due to time-constraints and package conflicts between Django_2.0/PostGIS/Heroku, I was unable to complete a delivered product that I could reliably host for end use. The application is functional locally, and the code can be viewed/downloaded here: \n",
    "\n",
    "[Passive Capture Python github repository](https://github.com/anadromous-data/passive_capture_py)\n",
    "\n",
    "As time allows, I hope to complete the application, and provide hosting and improvements. \n",
    "\n",
    "Although far from perfect, I hope to deliver a crude tool which enable time-series analysis and visualization to fish passage tracking efforts on the Columbia River. \n",
    "\n",
    "It's been a bunch of fun working with a new platform, and experimenting with statistical/GIS packages in python I may have not had the chance to use otherwise.\n",
    "\n",
    "objectives\n",
    "----------\n",
    "\n",
    "*   **To provide a tool which enables users to create custom time-series forecasts and visualizations for anadramous fish returns along the Columbia River.**\n",
    "\n",
    "*   **Compare the efficacy of Facebook's open source time series statistical package versus traditional ARIMA methods**\n",
    "\n",
    "technologies utilized\n",
    "---------------------\n",
    "\n",
    "*   Python 3\n",
    "*   GeoDjango\n",
    "*   QGIS 3\n",
    "*   BigQuery\n",
    "*   PostGIS\n",
    "*   Leaflet/Folium\n",
    "\n",
    "setup\n",
    "-----\n",
    "\n",
    "Below are my notes taken during the devleopment of this application\n",
    "\n",
    "I've taken a crack at fish passage modeling before, _(call it a hobby)_, and felt that using the US Army Core of Engineers (USACE) fish passage data for the Columbia River would be the most familiar and applicable for this project. The Fish Passage Center (fpc.org/http://www.fpc.org/documents/metadata/FPC\\_Adult\\_Metadata.html) provides a great interface to collect this data, and I've developed APIs in both [Ruby](https://github.com/anadromous-data/passive-capture) and [R](https://github.com/anadromous-data/fishpassagecenterr) to scrape this data.\n",
    "\n",
    "I wanted to work with a static set of data initially, as it would require less API overhead than to keep and API up and running to ETL the FPC data on a schedule\n",
    "\n",
    "**First Steps:**\n",
    "\n",
    "*   Downlaod a fresh set of passage / flow data for all dam sites ranging 2010 - 2018\n",
    "*   Create a job to load them into BigQuery\n",
    "*   Normalize and extract the data to CSV\n",
    "\n",
    "**Example Query to normalize the FPC Data Into BigQuery**\n",
    "\n",
    "```\n",
    "SELECT \n",
    "    PARSE_DATE(\"%m/%d/%Y\", date) AS count_date,\n",
    "    dam,\n",
    "    ChinookAdult AS chinook_adult,\n",
    "    ChinookJack AS chinook_jack,\n",
    "    CohoAdult AS coho_adult,\n",
    "    CohoJack AS coho_jack,\n",
    "    Steelhead AS steelhead,\n",
    "    WildSteelhead AS wild_steelhead,\n",
    "    Sockeye AS sockeye,\n",
    "    Pink AS pink,\n",
    "    Chum AS chum,\n",
    "    Lamprey AS lamprey,\n",
    "    Shad As shad\n",
    "FROM `passive-capture.passage_data.daily_counts`\n",
    "ORDER BY count_date;\n",
    "```\n",
    "\n",
    "Perhaps the most interesting aspect of Facebook's open source 'prophet' time-series forecasting package is the ability to incorporate additional regressors into model -- something that has traditionally been difficult in time-series analysis software. Additionally prophet comes with options to toggle 'seasonality detection', define custom seasonality trends, or add 'holidays'. Holidays are periods of time the model may expect additonal variance in trend, which seemed appropriate for runs of fish -- which occur seasonally.\n",
    "\n",
    "For autoregessors, I hope to examine the impact of current and historical precipitation and dam flows as impacts to the model. Precipiation in both rain and snow play a driving force in fry and parr survival rates, as well as the timing in which salmon species will either 'hold', or push on with their migration. Flows through the dams as well play a critical part in determining parr/smolt predation and survival rates.\n",
    "\n",
    "**Autogressors:**\n",
    "\n",
    "*   Current year/month precipitation totals\n",
    "*   Current year/month flows from Dam being modeled\n",
    "*   Current water-year snow/water equivalent standard deviation\n",
    "*   Historical (3yr) year/month precipitation totals\n",
    "*   Historical (3yr) year/month flows from Dam being modeled\n",
    "*   Historical (3yr) water-year snow/water equivalent standard deviation\n",
    "\n",
    "NOAA Data was extracted from public datasets -- metdata came from https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt\n",
    "\n",
    "**Example for snow data grouped by year month:** \n",
    "```\n",
    "SELECT\n",
    "    CONCAT( CAST(EXTRACT(YEAR FROM date) AS STRING),\n",
    "    CAST(EXTRACT(MONTH FROM date) AS STRING) ) AS year_month,\n",
    "    stations.name,\n",
    "    stations.latitude,\n",
    "    stations.longitude,\n",
    "    weather.id,\n",
    "    weather.element,\n",
    "    SUM(weather.value) AS snow_total,\n",
    "    AVG(DISTINCT weather.value) AS snow_average,\n",
    "    MIN(weather.value) AS snow_min,\n",
    "    MAX(weather.value) AS snow_max\n",
    "FROM `bigquery-public-data.ghcn_d.ghcnd_stations` AS stations JOIN `bigquery-public-data.ghcn_d.ghcnd_201*` AS weather ON weather.id = stations.id WHERE stations.latitude > 43 AND stations.latitude < 49 AND stations.longitude > -123 AND stations.longitude < -120 AND element = 'SNWD'\n",
    "GROUP BY year_month, stations.name, stations.latitude, stations.longitude, weather.id, weather.element\n",
    "ORDER BY year_month\n",
    "```\n",
    "\n",
    "statistical notes\n",
    "-----------------\n",
    "\n",
    "At present moment, there are 2 models I would like to pursue with this project. The first is a composite model between ARIMA and Prophet utilizing seasonality per distinct andaramous fish run and weather and flow features as autoregessors. The second would be ARIMA/Prophet time series analysis on each individual feature including run counts, flow, dissolved gas, precipitation etc -- and then compose a multivariate regression model using the predicted future values that may/may not contribute to run counts.\n",
    "\n",
    "I'm taking the log of the run counts because the variance in any given sample over the time series differs significantly. In log-log regression model it is the interpretation of estimated parameter, say Î±i as the elasticity of Y(t) on Xi(t). In error-correction models we have an empirically stronger assumption that proportions are more stable (stationary) than the absolute differences. It is easier to aggregate the log-returns over time.\n",
    "\n",
    "If you still want a statistical criterion for when to do log transformation a simple solution would be any test for heteroscedasticity. In the case of increasing variance I would recommend Goldfeld-Quandt Test or similar to it. In R it is located in library(lmtest) and is denoted by gqtest(y~1) function. Simply regress on intercept term if you don't have any regression model, y is your dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demonstration\n",
    "\n",
    "### forecast_generator.py\n",
    "\n",
    "The [`forecast_generator.py`](https://github.com/anadromous-data/passive_capture_py/blob/master/passive_capture/reporter/forecast_generator.py) script is where we download and clean the csv data for time series analysis. \n",
    "Below is a demonstration of registiring and outputting a single forecast for a species/dam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting the dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:35: RuntimeWarning: divide by zero encountered in log\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in log\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:37: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>2018-04-05</td>\n",
       "      <td>1.791759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>2018-04-06</td>\n",
       "      <td>3.295837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018</th>\n",
       "      <td>2018-04-07</td>\n",
       "      <td>2.890372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>1.791759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020</th>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ds         y\n",
       "3016 2018-04-05  1.791759\n",
       "3017 2018-04-06  3.295837\n",
       "3018 2018-04-07  2.890372\n",
       "3019 2018-04-08  1.791759\n",
       "3020 2018-04-09  1.386294"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# method to clean and format the dataframe for analysis. arguments are the CSV as formatted from FPC, the species to target,\n",
    "# and whether to display a 'count' of fish, or the log(count)\n",
    "\n",
    "csv = './csv/passage_data/bon_passage_data.csv'\n",
    "spp = 'chinook_adult'\n",
    "\n",
    "def create_dataframe(csv, spp, display_count=False):\n",
    "  print('Formatting the dataframe')\n",
    "  df = pd.read_csv(csv)\n",
    "\n",
    "  # isolate the dam\n",
    "  dam_name = df['dam'][0]\n",
    "\n",
    "  # select columns to remove to isolate a spp\n",
    "  cols_to_remove = [col for col in df.columns if f\"{spp}\" not in col and 'count_date' not in col]\n",
    "  df = df.drop(cols_to_remove, axis=1)\n",
    "  \n",
    "  # create date range to accommodate missing dates\n",
    "  df['count_date'] = pd.to_datetime(df['count_date'])\n",
    "  idx = pd.date_range(df['count_date'].iloc[0], df['count_date'].iloc[-1])\n",
    "  df.set_index('count_date',drop=True,inplace=True)\n",
    "  df = df.reindex(idx, fill_value=0).reset_index()\n",
    "\n",
    "  # for prophet, format to use ds and y cols\n",
    "  df['ds'] = df['index']\n",
    "\n",
    "  # flag return values to be either counts, or the log of the counts\n",
    "  if display_count:\n",
    "    df['y'] = df[f\"{spp}\"]\n",
    "  else:\n",
    "    df['y'] = np.log(df[f\"{spp}\"])\n",
    "    # replace all -inf \n",
    "    df['y'] = df['y'].replace([np.log(0)], 0)\n",
    "\n",
    "  df = df.drop(['index', f\"{spp}\"], axis=1)\n",
    "\n",
    "  return {'dataframe': df, 'dam': dam_name, 'spp': spp}\n",
    "\n",
    "# run the method:\n",
    "result_object = create_dataframe(csv,spp)\n",
    "df = result_object['dataframe']\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a dataframe which represents our range of data from the first date in the CSV (Jan 1st 2010) to the last (April 9th 2018). \n",
    "\n",
    "Some dates were missing so we added them into the dataframe, with 0 count for the fish count. That, as long as days where no fish passed, account for the error output stating that `divide by zero encountered in log`. This is due to `log(0)` being undefined (not a real number).\n",
    "\n",
    "So why did we decide to use the `log(count)` instead of the `count` to perform our time series analysis?\n",
    "\n",
    "The heteroscedasticy. As seen below, the minimum value in our set is -1 and the maximum value is 67,521. The variance between days can also be pretty significant, so let's run through a Breusch-Pagan test. \n",
    "\n",
    "We'll have to convert the `count_date` from a string to a numerical value to serve as the independent variable in this experiment. I've elected that month-date values should be most appropriate -- and I'll bar years. \n",
    "\n",
    "So `2018-03-30` will become `3.30`. `2010-04-14` will become `4.14`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n",
      "The minimum in the dataframe is -1  and the maximum is 67521\n",
      "####\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          chinook_adult   R-squared:                       0.173\n",
      "Model:                            OLS   Adj. R-squared:                  0.173\n",
      "Method:                 Least Squares   F-statistic:                     632.6\n",
      "Date:                Sun, 22 Apr 2018   Prob (F-statistic):          6.88e-127\n",
      "Time:                        21:16:26   Log-Likelihood:                -30183.\n",
      "No. Observations:                3018   AIC:                         6.037e+04\n",
      "Df Residuals:                    3017   BIC:                         6.037e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "count_date   329.7770     13.112     25.151      0.000     304.068     355.486\n",
      "==============================================================================\n",
      "Omnibus:                     3035.633   Durbin-Watson:                   0.121\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           176316.493\n",
      "Skew:                           4.913   Prob(JB):                         0.00\n",
      "Kurtosis:                      39.133   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv(csv)\n",
    "\n",
    "minimum = df['chinook_adult'].min()\n",
    "maximum = df['chinook_adult'].max()\n",
    "\n",
    "print('####')\n",
    "print('The minimum in the dataframe is', minimum, ' and the maximum is', maximum)\n",
    "print('####')\n",
    "\n",
    "date = df['count_date'].apply(lambda x: (datetime.strptime(x, '%Y-%m-%d').month + datetime.strptime(x, '%Y-%m-%d').day * .01) )\n",
    "\n",
    "spp_count = df['chinook_adult']\n",
    "\n",
    "results = smf.OLS(spp_count,date).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from fbprophet import Prophet\n",
    "\n",
    "\n",
    "def run_all_forecasts(spp, num_of_days=30):\n",
    "  for csv in passage_csvs():\n",
    "    run_forecast(csv, spp, num_of_days)\n",
    "\n",
    "def run_forecast(csv, spp, num_of_days=30, display_count=False):\n",
    "  df_opts = create_dataframe(csv, spp, display_count)\n",
    "  predict_passage(df_opts['dataframe'],df_opts['dam'],df_opts['spp'],num_of_days)\n",
    "\n",
    "def passage_csvs():\n",
    "  files = [os.path.abspath(f\"csv/passage_data/{x}\") for x in os.listdir('csv/passage_data')]\n",
    "  return files\n",
    "\n",
    "def create_dataframe(csv, spp, display_count=False):\n",
    "  print('Formatting the dataframe')\n",
    "  df = pd.read_csv(csv)\n",
    "\n",
    "  # isolate the dam\n",
    "  dam_name = df['dam'][0]\n",
    "\n",
    "  # select columns to remove to isolate a spp\n",
    "  cols_to_remove = [col for col in df.columns if f\"{spp}\" not in col and 'count_date' not in col]\n",
    "  df = df.drop(cols_to_remove, axis=1)\n",
    "  \n",
    "  # create date range to accommodate missing dates\n",
    "  df['count_date'] = pd.to_datetime(df['count_date'])\n",
    "  idx = pd.date_range(df['count_date'].iloc[0], df['count_date'].iloc[-1])\n",
    "  df.set_index('count_date',drop=True,inplace=True)\n",
    "  df = df.reindex(idx, fill_value=0).reset_index()\n",
    "\n",
    "  # for prophet, format to use ds and y cols\n",
    "  df['ds'] = df['index']\n",
    "\n",
    "  # flag return values to be either counts, or the log of the counts\n",
    "  if display_count:\n",
    "    df['y'] = df[f\"{spp}\"]\n",
    "  else:\n",
    "    df['y'] = np.log(df[f\"{spp}\"])\n",
    "    # replace all -inf \n",
    "    df['y'] = df['y'].replace([np.log(0)], 0)\n",
    "\n",
    "  df = df.drop(['index', f\"{spp}\"], axis=1)\n",
    "\n",
    "  return {'dataframe': df, 'dam': dam_name, 'spp': spp}\n",
    "\n",
    "def predict_passage(df,dam,spp,num_of_days):\n",
    "  print('Forecasting the future')\n",
    "  m = Prophet()\n",
    "  m.fit(df);\n",
    "  future = m.make_future_dataframe(periods=num_of_days)\n",
    "  forecast = m.predict(future)\n",
    "  grf = m.plot(forecast)\n",
    "  grf.savefig(f\"charts/{dam}_{spp}_forecast.png\")\n",
    "  forecast.to_csv(f\"csv/forecasts/{dam}_{spp}_forecast.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
